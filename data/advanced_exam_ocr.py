#!/usr/bin/env python3
"""
ë‚˜ë¬´ì˜ì‚¬ ê¸°ì¶œë¬¸ì œ ê³ í’ˆì§ˆ OCR ë° êµ¬ì¡°í™” ìŠ¤í¬ë¦½íŠ¸
- 300 DPI ì´ìƒ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³€í™˜
- í•œêµ­ì–´ íŠ¹í™” OCR (Tesseract Korean)
- ì§€ëŠ¥í˜• íŒ¨í„´ ì¸ì‹ ë° êµ¬ì¡°í™”
"""
import os
import re
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import io

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    import fitz  # PyMuPDF
    from PIL import Image, ImageEnhance, ImageFilter
    import pytesseract
    import numpy as np
    import cv2
except ImportError as e:
    print(f"í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("pip install PyMuPDF pillow pytesseract numpy opencv-python")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('exam_ocr.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AdvancedExamOCR:
    """ê³ í’ˆì§ˆ ë‚˜ë¬´ì˜ì‚¬ ê¸°ì¶œë¬¸ì œ OCR ì²˜ë¦¬ê¸°"""
    
    def __init__(self, pdf_path: str, output_path: str):
        self.pdf_path = pdf_path
        self.output_path = output_path
        self.exam_round = self.extract_exam_round(pdf_path)
        self.questions = []
        self.failed_pages = []
        self.ocr_stats = {
            'total_pages': 0,
            'ocr_pages': 0,
            'text_pages': 0,
            'total_chars': 0
        }
        
    def extract_exam_round(self, pdf_path: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ íšŒì°¨ ì¶”ì¶œ"""
        match = re.search(r'ì œ(\d+)íšŒ', pdf_path)
        return match.group(1) if match else "Unknown"
    
    def process_pdf(self) -> Dict:
        """PDF ì „ì²´ ì²˜ë¦¬ (ê³ í’ˆì§ˆ OCR)"""
        logger.info(f"ê³ í’ˆì§ˆ OCR ì²˜ë¦¬ ì‹œì‘: {self.pdf_path}")
        
        try:
            pdf = fitz.open(self.pdf_path)
            self.ocr_stats['total_pages'] = len(pdf)
            
            all_text = ""  # ì „ì²´ í…ìŠ¤íŠ¸ ëˆ„ì 
            
            for page_num in range(len(pdf)):
                try:
                    logger.info(f"í˜ì´ì§€ {page_num + 1}/{len(pdf)} ì²˜ë¦¬ ì¤‘...")
                    page = pdf[page_num]
                    
                    # ë¨¼ì € í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    text = page.get_text()
                    
                    # í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ê³ í’ˆì§ˆ OCR ìˆ˜í–‰
                    if len(text.strip()) < 200 or self._is_image_based_page(page):
                        logger.info(f"í˜ì´ì§€ {page_num + 1}: ê³ í’ˆì§ˆ OCR ìˆ˜í–‰")
                        text = self.perform_high_quality_ocr(page)
                        self.ocr_stats['ocr_pages'] += 1
                    else:
                        self.ocr_stats['text_pages'] += 1
                    
                    self.ocr_stats['total_chars'] += len(text)
                    all_text += f"\n\n=== í˜ì´ì§€ {page_num + 1} ===\n\n{text}"
                    
                except Exception as e:
                    logger.error(f"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    self.failed_pages.append({
                        'page': page_num + 1,
                        'error': str(e)
                    })
            
            pdf.close()
            
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œ ì¶”ì¶œ
            self.extract_all_questions(all_text)
            
            # ê²°ê³¼ ì €ì¥
            self.save_to_markdown()
            
            return {
                'exam_round': self.exam_round,
                'total_questions': len(self.questions),
                'failed_pages': self.failed_pages,
                'ocr_stats': self.ocr_stats,
                'output_file': self.output_path
            }
            
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _is_image_based_page(self, page) -> bool:
        """í˜ì´ì§€ê°€ ì´ë¯¸ì§€ ê¸°ë°˜ì¸ì§€ í™•ì¸"""
        # ì´ë¯¸ì§€ ê°œìˆ˜ì™€ í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ íŒë‹¨
        image_list = page.get_images()
        text_len = len(page.get_text().strip())
        return len(image_list) > 0 and text_len < 100
    
    def perform_high_quality_ocr(self, page) -> str:
        """ê³ í’ˆì§ˆ OCR ìˆ˜í–‰ (300 DPI)"""
        # 300 DPIë¡œ ì´ë¯¸ì§€ ë³€í™˜
        mat = fitz.Matrix(300/72, 300/72)  # 300 DPI
        pix = page.get_pixmap(matrix=mat, alpha=False)
        img_data = pix.tobytes("png")
        
        # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        img = Image.open(io.BytesIO(img_data))
        
        # ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img = self.advanced_preprocess_image(img)
        
        # OCR ìˆ˜í–‰ (í•œêµ­ì–´, ê³ í’ˆì§ˆ ì„¤ì •)
        custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ê°€-í£ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzâ‘ â‘¡â‘¢â‘£â‘¤()[].,!?:;-_ '
        text = pytesseract.image_to_string(img, lang='kor', config=custom_config)
        
        return text
    
    def advanced_preprocess_image(self, img: Image) -> Image:
        """ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # OpenCVë¡œ ë³€í™˜
        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        
        # ë…¸ì´ì¦ˆ ì œê±°
        denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
        
        # ì ì‘í˜• ì„ê³„ê°’ ì²˜ë¦¬
        binary = cv2.adaptiveThreshold(
            denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 11, 2
        )
        
        # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ê°œì„ 
        kernel = np.ones((1, 1), np.uint8)
        morphed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        
        # PIL ì´ë¯¸ì§€ë¡œ ì¬ë³€í™˜
        img_pil = Image.fromarray(morphed)
        
        # ì„ ëª…ë„ í–¥ìƒ
        enhancer = ImageEnhance.Sharpness(img_pil)
        img_pil = enhancer.enhance(2.0)
        
        return img_pil
    
    def extract_all_questions(self, full_text: str):
        """ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ë¬¸ì œ ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´
        question_patterns = [
            r'(\d{1,3})\.\s*([^\n]+.*?)(?=(?:\d{1,3}\.|ì •ë‹µ|í•´ì„¤|$))',  # ê¸°ë³¸ íŒ¨í„´
            r'ë¬¸ì œ\s*(\d{1,3})[.)]?\s*([^\n]+.*?)(?=(?:ë¬¸ì œ\s*\d{1,3}|ì •ë‹µ|í•´ì„¤|$))',  # ë¬¸ì œ N
            r'ã€ë¬¸ì œ(\d{1,3})ã€‘\s*([^\n]+.*?)(?=(?:ã€ë¬¸ì œ\d{1,3}ã€‘|ì •ë‹µ|í•´ì„¤|$))',  # ã€ë¬¸ì œNã€‘
            r'Q(\d{1,3})[.)]?\s*([^\n]+.*?)(?=(?:Q\d{1,3}|ì •ë‹µ|í•´ì„¤|$))',  # QN
        ]
        
        all_questions = []
        
        for pattern in question_patterns:
            matches = list(re.finditer(pattern, full_text, re.DOTALL | re.MULTILINE))
            for match in matches:
                question_num = int(match.group(1))
                question_content = match.group(2).strip()
                
                # ì¤‘ë³µ ì²´í¬
                if not any(q['number'] == question_num for q in all_questions):
                    all_questions.append({
                        'number': question_num,
                        'content': question_content,
                        'start': match.start(),
                        'end': match.end()
                    })
        
        # ë²ˆí˜¸ìˆœ ì •ë ¬
        all_questions.sort(key=lambda x: x['number'])
        
        # ê° ë¬¸ì œì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        for q in all_questions:
            self.extract_question_details(q, full_text)
        
        self.questions = all_questions
        logger.info(f"ì´ {len(self.questions)}ê°œ ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ")
    
    def extract_question_details(self, question: Dict, full_text: str):
        """ë¬¸ì œì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        # ë¬¸ì œ ë²”ìœ„ ê²°ì •
        start = question['start']
        next_q = next((q for q in self.questions if q['number'] == question['number'] + 1), None)
        end = next_q['start'] if next_q else len(full_text)
        
        question_text = full_text[start:end]
        
        # ì„ íƒì§€ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´)
        choices = self.extract_choices(question_text)
        
        # ì •ë‹µ ì¶”ì¶œ
        answer = self.extract_answer(question_text)
        
        # í•´ì„¤ ì¶”ì¶œ
        explanation = self.extract_explanation(question_text)
        
        # ê³¼ëª© ì¶”ì¶œ
        subject = self.extract_subject(question_text)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(question_text)
        
        # ë¬¸ì œ ì •ë³´ ì—…ë°ì´íŠ¸
        question.update({
            'question': question['content'],
            'choices': choices,
            'answer': answer,
            'explanation': explanation,
            'subject': subject,
            'keywords': keywords
        })
        
        # ì„ì‹œ í•„ë“œ ì œê±°
        del question['content']
        del question['start']
        del question['end']
    
    def extract_choices(self, text: str) -> Dict[str, str]:
        """ì„ íƒì§€ ì¶”ì¶œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)"""
        choices = {}
        
        # íŒ¨í„´ 1: â‘  â‘¡ â‘¢ â‘£ â‘¤
        pattern1 = r'([â‘ â‘¡â‘¢â‘£â‘¤])\s*([^\nâ‘ â‘¡â‘¢â‘£â‘¤]+)'
        
        # íŒ¨í„´ 2: 1) 2) 3) 4) 5)
        pattern2 = r'(\d)\)\s*([^\n\d)]+)'
        
        # íŒ¨í„´ 3: (1) (2) (3) (4) (5)
        pattern3 = r'\((\d)\)\s*([^\n\(\)]+)'
        
        # íŒ¨í„´ 4: ã„±. ã„´. ã„·. ã„¹.
        pattern4 = r'([ã„±ã„´ã„·ã„¹ã…ã…‚ã……ã…‡ã…ˆã…Šã…‹ã…Œã…ã…])\.\s*([^\nã„±ã„´ã„·ã„¹ã…ã…‚ã……ã…‡ã…ˆã…Šã…‹ã…Œã…ã…]+)'
        
        for pattern in [pattern1, pattern2, pattern3, pattern4]:
            matches = re.finditer(pattern, text, re.MULTILINE)
            for match in matches:
                key = match.group(1)
                value = match.group(2).strip()
                if value and len(value) > 2:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ
                    choices[key] = value
        
        return choices
    
    def extract_answer(self, text: str) -> str:
        """ì •ë‹µ ì¶”ì¶œ"""
        answer_patterns = [
            r'ì •ë‹µ\s*[:ï¼š]\s*([â‘ â‘¡â‘¢â‘£â‘¤\d]+)',
            r'ë‹µ\s*[:ï¼š]\s*([â‘ â‘¡â‘¢â‘£â‘¤\d]+)',
            r'ã€ì •ë‹µã€‘\s*([â‘ â‘¡â‘¢â‘£â‘¤\d]+)',
            r'Answer\s*[:ï¼š]\s*([â‘ â‘¡â‘¢â‘£â‘¤\d]+)',
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return "ë¯¸í™•ì¸"
    
    def extract_explanation(self, text: str) -> str:
        """í•´ì„¤ ì¶”ì¶œ"""
        explanation_patterns = [
            r'í•´ì„¤\s*[:ï¼š]\s*(.*?)(?=ë¬¸ì œ\s*\d|$)',
            r'ì„¤ëª…\s*[:ï¼š]\s*(.*?)(?=ë¬¸ì œ\s*\d|$)',
            r'ã€í•´ì„¤ã€‘\s*(.*?)(?=ë¬¸ì œ\s*\d|$)',
            r'í’€ì´\s*[:ï¼š]\s*(.*?)(?=ë¬¸ì œ\s*\d|$)',
        ]
        
        for pattern in explanation_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                explanation = match.group(1).strip()
                # ë„ˆë¬´ ê¸´ í•´ì„¤ì€ ì ì ˆíˆ ìë¥´ê¸°
                if len(explanation) > 1000:
                    explanation = explanation[:1000] + "..."
                return explanation
        
        return "í•´ì„¤ ì—†ìŒ"
    
    def extract_subject(self, text: str) -> str:
        """ê³¼ëª© ì¶”ì¶œ"""
        subjects = {
            'ìˆ˜ëª©ë³‘ë¦¬í•™': ['ë³‘í•´', 'ë³‘ì›ê· ', 'ê³°íŒ¡ì´', 'ì„¸ê· ', 'ë°”ì´ëŸ¬ìŠ¤'],
            'ìˆ˜ëª©í•´ì¶©í•™': ['í•´ì¶©', 'ê³¤ì¶©', 'ì²œì ', 'ë°©ì œ'],
            'ìˆ˜ëª©ìƒë¦¬í•™': ['ê´‘í•©ì„±', 'í˜¸í¡', 'ì¦ì‚°', 'ì–‘ë¶„'],
            'í† ì–‘í•™': ['í† ì–‘', 'pH', 'ì–‘ë¶„', 'ë¹„ë£Œ'],
            'ìˆ˜ëª©ê´€ë¦¬í•™': ['ì „ì •', 'ê°€ì§€ì¹˜ê¸°', 'ì‹ì¬', 'ì´ì‹'],
            'ë†ì•½í•™': ['ë†ì•½', 'ì‚´ì¶©ì œ', 'ì‚´ê· ì œ', 'ì œì´ˆì œ'],
        }
        
        text_lower = text.lower()
        scores = {}
        
        for subject, keywords in subjects.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                scores[subject] = score
        
        if scores:
            return max(scores, key=scores.get)
        return "ì¼ë°˜"
    
    def extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ (í–¥ìƒëœ ë²„ì „)"""
        keywords = set()
        
        # ì „ë¬¸ ìš©ì–´ ì‚¬ì „
        term_dict = {
            'ë³‘ì¶©í•´': ['ë³‘í•´', 'ì¶©í•´', 'ë³‘ì¶©í•´', 'ë³‘ì›ê· ', 'í•´ì¶©'],
            'ë°©ì œ': ['ë°©ì œ', 'ì‚´ì¶©ì œ', 'ì‚´ê· ì œ', 'ë†ì•½', 'ì•½ì œ'],
            'ì§„ë‹¨': ['ì§„ë‹¨', 'ì¦ìƒ', 'ë³‘ì§•', 'í‘œì§•', 'í”¼í•´'],
            'ìˆ˜ëª©ê´€ë¦¬': ['ì „ì •', 'ê°€ì§€ì¹˜ê¸°', 'ì „ì§€', 'ìˆ˜í˜•', 'ì •ì§€'],
            'í† ì–‘': ['í† ì–‘', 'pH', 'ë¹„ë£Œ', 'ì–‘ë¶„', 'ì˜ì–‘'],
            'ìƒë¦¬': ['ê´‘í•©ì„±', 'í˜¸í¡', 'ì¦ì‚°', 'ìˆ˜ë¶„'],
        }
        
        # ì „ë¬¸ ìš©ì–´ ê²€ìƒ‰
        for category, terms in term_dict.items():
            if any(term in text for term in terms):
                keywords.add(category)
        
        # ìˆ˜ì¢… ì¶”ì¶œ
        tree_pattern = r'(ì†Œë‚˜ë¬´|ì£ë‚˜ë¬´|ì€í–‰ë‚˜ë¬´|ëŠí‹°ë‚˜ë¬´|ë²šë‚˜ë¬´|ë‹¨í’ë‚˜ë¬´|ì°¸ë‚˜ë¬´|ë°¤ë‚˜ë¬´|ê°ë‚˜ë¬´|ë°°ë‚˜ë¬´|ì‚¬ê³¼ë‚˜ë¬´)'
        trees = re.findall(tree_pattern, text)
        keywords.update(set(trees))
        
        # ë³‘í•´ì¶©ëª… ì¶”ì¶œ
        pest_pattern = r'([ê°€-í£]+ë³‘|[ê°€-í£]+ì¶©|[ê°€-í£]+ê· )'
        pests = re.findall(pest_pattern, text)
        keywords.update(set(pests[:3]))  # ìƒìœ„ 3ê°œë§Œ
        
        return list(keywords)
    
    def save_to_markdown(self):
        """êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥"""
        content = f"# ë‚˜ë¬´ì˜ì‚¬ ì œ{self.exam_round}íšŒ ê¸°ì¶œë¬¸ì œ (ì™„ë²½ ë³µì›íŒ)\n\n"
        content += f"**ì´ ë¬¸ì œ ìˆ˜**: {len(self.questions)}ê°œ\n"
        content += f"**ì¶”ì¶œ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += f"**OCR í’ˆì§ˆ**: ê³ í•´ìƒë„ 300 DPI\n"
        content += f"**ì²˜ë¦¬ í†µê³„**:\n"
        content += f"- ì „ì²´ í˜ì´ì§€: {self.ocr_stats['total_pages']}í˜ì´ì§€\n"
        content += f"- OCR ì²˜ë¦¬: {self.ocr_stats['ocr_pages']}í˜ì´ì§€\n"
        content += f"- í…ìŠ¤íŠ¸ ì¶”ì¶œ: {self.ocr_stats['text_pages']}í˜ì´ì§€\n"
        content += f"- ì „ì²´ ë¬¸ììˆ˜: {self.ocr_stats['total_chars']:,}ì\n\n"
        content += "---\n\n"
        
        # ê³¼ëª©ë³„ ë¶„ë¥˜
        subjects = {}
        for q in self.questions:
            subject = q.get('subject', 'ì¼ë°˜')
            if subject not in subjects:
                subjects[subject] = []
            subjects[subject].append(q)
        
        # ê³¼ëª©ë³„ë¡œ ì¶œë ¥
        for subject, questions in subjects.items():
            content += f"## ğŸ“š {subject} ({len(questions)}ë¬¸ì œ)\n\n"
            
            for q in sorted(questions, key=lambda x: x['number']):
                content += f"### ë¬¸ì œ {q['number']}\n"
                if q.get('subject') != 'ì¼ë°˜':
                    content += f"**ê³¼ëª©**: {q['subject']}\n"
                content += f"**ë¬¸ì œ**: {q['question']}\n\n"
                
                if q['choices']:
                    content += "**ì„ íƒì§€**:\n"
                    for key in ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', '1', '2', '3', '4', '5']:
                        if key in q['choices']:
                            content += f"{key} {q['choices'][key]}\n"
                    content += "\n"
                
                content += f"**ì •ë‹µ**: {q['answer']}\n\n"
                
                if q['explanation'] != "í•´ì„¤ ì—†ìŒ":
                    content += f"**í•´ì„¤**: {q['explanation']}\n\n"
                
                if q['keywords']:
                    content += f"**í‚¤ì›Œë“œ**: {', '.join(q['keywords'])}\n\n"
                
                content += "---\n\n"
        
        # ì²˜ë¦¬ ì‹¤íŒ¨ ì •ë³´
        if self.failed_pages:
            content += "\n## âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨ í˜ì´ì§€\n\n"
            for failed in self.failed_pages:
                content += f"- í˜ì´ì§€ {failed['page']}: {failed['error']}\n"
            content += "\n"
        
        # í’ˆì§ˆ ë³´ì¦ ì •ë³´
        content += "\n## ğŸ“Š í’ˆì§ˆ ë³´ì¦\n\n"
        content += f"- ë¬¸ì œ ì¶”ì¶œë¥ : {len(self.questions)/150*100:.1f}% (ì˜ˆìƒ 150ë¬¸ì œ ê¸°ì¤€)\n"
        content += f"- ì„ íƒì§€ ì™„ì„±ë„: {sum(1 for q in self.questions if len(q['choices']) >= 4)/len(self.questions)*100:.1f}%\n"
        content += f"- ì •ë‹µ í™•ì¸ë¥ : {sum(1 for q in self.questions if q['answer'] != 'ë¯¸í™•ì¸')/len(self.questions)*100:.1f}%\n"
        content += f"- í•´ì„¤ í¬í•¨ë¥ : {sum(1 for q in self.questions if q['explanation'] != 'í•´ì„¤ ì—†ìŒ')/len(self.questions)*100:.1f}%\n"
        
        # íŒŒì¼ ì €ì¥
        with open(self.output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"ê³ í’ˆì§ˆ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {self.output_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì²˜ë¦¬í•  PDF íŒŒì¼
    pdf_files = [
        {
            'input': '/Users/voidlight/Downloads/[Codekiller]ì œ5íšŒ ê¸°ì¶œë¬¸ì œ í•´ì„¤ì§‘(v1.0).pdf',
            'output': '/Users/voidlight/tree-doctor-pdf-qa-mcp/data/exam-5th-complete.md'
        },
        {
            'input': '/Users/voidlight/Downloads/[Codekiller]ì œ6íšŒ ê¸°ì¶œë¬¸ì œ í•´ì„¤ì§‘(v1.0).pdf',
            'output': '/Users/voidlight/tree-doctor-pdf-qa-mcp/data/exam-6th-complete.md'
        }
    ]
    
    results = []
    
    for file_info in pdf_files:
        if not os.path.exists(file_info['input']):
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_info['input']}")
            continue
        
        try:
            processor = AdvancedExamOCR(file_info['input'], file_info['output'])
            result = processor.process_pdf()
            results.append(result)
            
            logger.info(f"âœ… ì œ{result['exam_round']}íšŒ ì²˜ë¦¬ ì™„ë£Œ: {result['total_questions']}ë¬¸ì œ")
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {file_info['input']} - {e}")
            results.append({
                'exam_round': 'Unknown',
                'error': str(e),
                'input_file': file_info['input']
            })
    
    # ìµœì¢… ë³´ê³ ì„œ
    report_path = '/Users/voidlight/tree-doctor-pdf-qa-mcp/data/advanced_ocr_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ìš”ì•½ ì¶œë ¥
    print("\n=== ğŸš€ ê³ í’ˆì§ˆ OCR ì²˜ë¦¬ ì™„ë£Œ ===")
    for result in results:
        if 'error' in result:
            print(f"âŒ ì œ{result['exam_round']}íšŒ: ì²˜ë¦¬ ì‹¤íŒ¨")
        else:
            print(f"âœ… ì œ{result['exam_round']}íšŒ: {result['total_questions']}ë¬¸ì œ ì¶”ì¶œ")
            stats = result['ocr_stats']
            print(f"   - OCR ì²˜ë¦¬: {stats['ocr_pages']}/{stats['total_pages']} í˜ì´ì§€")
            print(f"   - ì „ì²´ ë¬¸ì: {stats['total_chars']:,}ì")

if __name__ == "__main__":
    main()